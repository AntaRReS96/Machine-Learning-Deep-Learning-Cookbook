{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Random Forest**  \n",
        "### *Las losowy*\n",
        "\n",
        "---\n",
        "\n",
        "## **English**\n",
        "\n",
        "Random Forest is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. It uses the principles of bagging (bootstrap aggregating) and random feature selection to reduce overfitting and improve generalization.\n",
        "\n",
        "### **Key Concepts**\n",
        "\n",
        "1. **Bootstrap Sampling**: Each tree is trained on a random subset of the data (with replacement)\n",
        "2. **Random Feature Selection**: At each split, only a random subset of features is considered\n",
        "3. **Voting/Averaging**: Final prediction is made by majority vote (classification) or averaging (regression)\n",
        "\n",
        "### **Algorithm Steps**\n",
        "\n",
        "1. **Bootstrap**: Create multiple bootstrap samples from the training data\n",
        "2. **Train Trees**: Train a decision tree on each bootstrap sample\n",
        "3. **Random Features**: For each split, consider only a random subset of features\n",
        "4. **Aggregate**: Combine predictions from all trees\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "- **Reduced Overfitting**: Ensemble approach reduces variance\n",
        "- **Feature Importance**: Built-in feature importance calculation\n",
        "- **Handles Missing Values**: Can work with incomplete data\n",
        "- **Parallel Training**: Trees can be trained independently\n",
        "\n",
        "### **Hyperparameters**\n",
        "\n",
        "- `n_estimators`: Number of trees in the forest\n",
        "- `max_depth`: Maximum depth of individual trees\n",
        "- `max_features`: Number of features to consider at each split\n",
        "- `min_samples_split`: Minimum samples required to split a node\n",
        "\n",
        "---\n",
        "\n",
        "## **Polish**\n",
        "\n",
        "Las losowy to metoda uczenia zespołowego, która łączy wiele drzew decyzyjnych w celu stworzenia bardziej odpornego i dokładnego modelu. Wykorzystuje zasady baggingu (bootstrap aggregating) i losowego wyboru cech, aby zmniejszyć przeuczenie i poprawić generalizację.\n",
        "\n",
        "### **Kluczowe pojęcia**\n",
        "\n",
        "1. **Próbkowanie bootstrap**: Każde drzewo jest trenowane na losowym podzbiorze danych (z powtórzeniami)\n",
        "2. **Losowy wybór cech**: W każdym podziale rozważany jest tylko losowy podzbiór cech\n",
        "3. **Głosowanie/Uśrednianie**: Końcowa predykcja jest tworzona przez głosowanie większości (klasyfikacja) lub uśrednianie (regresja)\n",
        "\n",
        "### **Kroki algorytmu**\n",
        "\n",
        "1. **Bootstrap**: Tworzenie wielu próbek bootstrap z danych treningowych\n",
        "2. **Trenowanie drzew**: Trenowanie drzewa decyzyjnego na każdej próbce bootstrap\n",
        "3. **Losowe cechy**: Dla każdego podziału rozważenie tylko losowego podzbioru cech\n",
        "4. **Agregacja**: Łączenie predykcji ze wszystkich drzew\n",
        "\n",
        "### **Zalety**\n",
        "\n",
        "- **Zmniejszone przeuczenie**: Podejście zespołowe redukuje wariancję\n",
        "- **Ważność cech**: Wbudowane obliczanie ważności cech\n",
        "- **Obsługuje brakujące wartości**: Może pracować z niepełnymi danymi\n",
        "- **Równoległe trenowanie**: Drzewa mogą być trenowane niezależnie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape | Kształt zbioru danych: (1000, 20)\n",
            "Number of classes | Liczba klas: 3\n",
            "Training set size | Rozmiar zbioru treningowego: 800\n",
            "Test set size | Rozmiar zbioru testowego: 200\n",
            "\n",
            "Decision Tree Accuracy | Dokładność drzewa decyzyjnego: 0.660\n",
            "Random Forest Accuracy | Dokładność lasu losowego: 0.805\n",
            "Improvement | Poprawa: 0.145\n",
            "\n",
            "Random Forest Details | Szczegóły lasu losowego:\n",
            "Number of trees | Liczba drzew: 100\n",
            "Number of features considered per split | Liczba cech na podział: sqrt\n",
            "OOB not calculated\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, validation_curve\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification, load_wine\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a complex classification dataset\n",
        "# Generowanie złożonego zbioru danych klasyfikacyjnych\n",
        "np.random.seed(42)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, \n",
        "                          n_redundant=10, n_classes=3, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "# Podział danych\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Dataset shape | Kształt zbioru danych: {X.shape}\")\n",
        "print(f\"Number of classes | Liczba klas: {len(np.unique(y))}\")\n",
        "print(f\"Training set size | Rozmiar zbioru treningowego: {X_train.shape[0]}\")\n",
        "print(f\"Test set size | Rozmiar zbioru testowego: {X_test.shape[0]}\")\n",
        "\n",
        "# Compare single decision tree vs Random Forest\n",
        "# Porównanie pojedynczego drzewa decyzyjnego z lasem losowym\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f\"\\nDecision Tree Accuracy | Dokładność drzewa decyzyjnego: {dt_accuracy:.3f}\")\n",
        "print(f\"Random Forest Accuracy | Dokładność lasu losowego: {rf_accuracy:.3f}\")\n",
        "print(f\"Improvement | Poprawa: {rf_accuracy - dt_accuracy:.3f}\")\n",
        "\n",
        "print(f\"\\nRandom Forest Details | Szczegóły lasu losowego:\")\n",
        "print(f\"Number of trees | Liczba drzew: {rf.n_estimators}\")\n",
        "print(f\"Number of features considered per split | Liczba cech na podział: {rf.max_features}\")\n",
        "print(f\"Out-of-bag score | Wynik out-of-bag: {rf.oob_score_:.3f}\" if hasattr(rf, 'oob_score_') else \"OOB not calculated\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mldl-cookbook",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
