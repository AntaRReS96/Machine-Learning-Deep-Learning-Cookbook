{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Support Vector Machines (SVM)**  \n",
        "### *Maszyny wektorów nośnych*\n",
        "\n",
        "---\n",
        "\n",
        "## **English**\n",
        "\n",
        "Support Vector Machines (SVMs) are powerful, versatile machine learning models capable of performing linear or nonlinear classification, regression, and even outlier detection. They are particularly well suited for classification of complex small- or medium-sized datasets.\n",
        "\n",
        "### **Key Concepts**\n",
        "\n",
        "1. **Support Vectors**: Training instances that lie closest to the decision boundary\n",
        "2. **Hyperplane**: Decision boundary that separates classes in feature space\n",
        "3. **Margin**: Distance between the decision boundary and closest training instances\n",
        "4. **Kernel Trick**: Technique to efficiently compute dot products in high-dimensional space\n",
        "5. **Soft Margin**: Allows some misclassifications to handle non-separable data\n",
        "\n",
        "### **Mathematical Foundation**\n",
        "\n",
        "#### **Linear SVM Decision Function**\n",
        "The decision function for a linear SVM is:\n",
        "$$\n",
        "h_{\\mathbf{w}, b}(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{w}$ is the weight vector\n",
        "- $b$ is the bias term\n",
        "- Classification: $\\hat{y} = \\text{sign}(h_{\\mathbf{w}, b}(\\mathbf{x}))$\n",
        "\n",
        "#### **Optimization Objective**\n",
        "SVM aims to maximize the margin by minimizing:\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "$$\n",
        "y^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\geq 1, \\quad \\forall i\n",
        "$$\n",
        "\n",
        "#### **Soft Margin SVM**\n",
        "For non-separable data, we introduce slack variables $\\zeta^{(i)}$:\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b, \\zeta} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{m} \\zeta^{(i)}\n",
        "$$\n",
        "\n",
        "Where $C$ is the regularization parameter controlling the trade-off between margin maximization and classification errors.\n",
        "\n",
        "### **Kernel Functions**\n",
        "\n",
        "#### **Linear Kernel**\n",
        "$$\n",
        "K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = \\mathbf{x}^{(i)T} \\mathbf{x}^{(j)}\n",
        "$$\n",
        "\n",
        "#### **Polynomial Kernel**\n",
        "$$\n",
        "K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = (\\gamma \\mathbf{x}^{(i)T} \\mathbf{x}^{(j)} + r)^d\n",
        "$$\n",
        "\n",
        "#### **Gaussian RBF Kernel**\n",
        "$$\n",
        "K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = \\exp(-\\gamma \\|\\mathbf{x}^{(i)} - \\mathbf{x}^{(j)}\\|^2)\n",
        "$$\n",
        "\n",
        "### **Types of SVM**\n",
        "\n",
        "1. **Linear SVM**: For linearly separable data\n",
        "2. **Nonlinear SVM**: Uses kernel trick for non-linear boundaries\n",
        "3. **SVM Regression (SVR)**: For regression tasks\n",
        "4. **Nu-SVM**: Alternative formulation with different parameterization\n",
        "\n",
        "### **Advantages**\n",
        "- **Memory Efficient**: Uses subset of training points (support vectors)\n",
        "- **Versatile**: Different kernel functions for different data types\n",
        "- **Works Well**: Effective in high-dimensional spaces\n",
        "- **Robust**: Less prone to overfitting with proper regularization\n",
        "\n",
        "### **Disadvantages**\n",
        "- **No Probabilistic Output**: Doesn't provide probability estimates directly\n",
        "- **Sensitive to Feature Scaling**: Requires feature normalization\n",
        "- **Slow on Large Datasets**: Computational complexity is high for large datasets\n",
        "- **Hyperparameter Sensitivity**: Performance depends on proper parameter tuning\n",
        "\n",
        "---\n",
        "\n",
        "## **Polish**\n",
        "\n",
        "Maszyny wektorów nośnych (SVM) to potężne, wszechstronne modele uczenia maszynowego zdolne do wykonywania klasyfikacji liniowej lub nieliniowej, regresji, a nawet wykrywania wartości odstających. Są szczególnie dobrze dostosowane do klasyfikacji złożonych małych lub średnich zbiorów danych.\n",
        "\n",
        "### **Kluczowe pojęcia**\n",
        "\n",
        "1. **Wektory nośne**: Instancje treningowe leżące najbliżej granicy decyzyjnej\n",
        "2. **Hiperpłaszczyzna**: Granica decyzyjna oddzielająca klasy w przestrzeni cech\n",
        "3. **Margines**: Odległość między granicą decyzyjną a najbliższymi instancjami treningowymi\n",
        "4. **Trik jądrowy**: Technika efektywnego obliczania iloczynów skalarnych w przestrzeni wysokowymiarowej\n",
        "5. **Miękki margines**: Pozwala na niektóre błędne klasyfikacje dla danych nierozdzielnych\n",
        "\n",
        "### **Podstawy matematyczne**\n",
        "\n",
        "#### **Funkcja decyzyjna liniowego SVM**\n",
        "Funkcja decyzyjna dla liniowego SVM to:\n",
        "$$\n",
        "h_{\\mathbf{w}, b}(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "Gdzie:\n",
        "- $\\mathbf{w}$ to wektor wag\n",
        "- $b$ to składnik bias\n",
        "- Klasyfikacja: $\\hat{y} = \\text{sign}(h_{\\mathbf{w}, b}(\\mathbf{x}))$\n",
        "\n",
        "#### **Cel optymalizacji**\n",
        "SVM dąży do maksymalizacji marginesu przez minimalizację:\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
        "$$\n",
        "\n",
        "Przy ograniczeniach:\n",
        "$$\n",
        "y^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\geq 1, \\quad \\forall i\n",
        "$$\n",
        "\n",
        "#### **SVM z miękkim marginesem**\n",
        "Dla danych nierozdzielnych wprowadzamy zmienne luzu $\\zeta^{(i)}$:\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b, \\zeta} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{m} \\zeta^{(i)}\n",
        "$$\n",
        "\n",
        "Gdzie $C$ to parametr regularyzacji kontrolujący kompromis między maksymalizacją marginesu a błędami klasyfikacji.\n",
        "\n",
        "### **Funkcje jądrowe**\n",
        "\n",
        "#### **Jądro liniowe**\n",
        "$$\n",
        "K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = \\mathbf{x}^{(i)T} \\mathbf{x}^{(j)}\n",
        "$$\n",
        "\n",
        "#### **Jądro wielomianowe**\n",
        "$$\n",
        "K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = (\\gamma \\mathbf{x}^{(i)T} \\mathbf{x}^{(j)} + r)^d\n",
        "$$\n",
        "\n",
        "#### **Jądro Gaussowskie RBF**\n",
        "$$\n",
        "K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = \\exp(-\\gamma \\|\\mathbf{x}^{(i)} - \\mathbf{x}^{(j)}\\|^2)\n",
        "$$\n",
        "\n",
        "### **Typy SVM**\n",
        "\n",
        "1. **Liniowy SVM**: Dla danych liniowo rozdzielnych\n",
        "2. **Nieliniowy SVM**: Używa triku jądrowego dla nieliniowych granic\n",
        "3. **Regresja SVM (SVR)**: Do zadań regresji\n",
        "4. **Nu-SVM**: Alternatywne sformułowanie z inną parametryzacją\n",
        "\n",
        "### **Zalety**\n",
        "- **Efektywność pamięciowa**: Używa podzbioru punktów treningowych (wektory nośne)\n",
        "- **Wszechstronność**: Różne funkcje jądrowe dla różnych typów danych\n",
        "- **Działa dobrze**: Skuteczny w przestrzeniach wysokowymiarowych\n",
        "- **Odporność**: Mniej skłonny do przeuczenia przy odpowiedniej regularyzacji\n",
        "\n",
        "### **Wady**\n",
        "- **Brak wyjścia probabilistycznego**: Nie dostarcza bezpośrednio oszacowań prawdopodobieństwa\n",
        "- **Wrażliwość na skalowanie cech**: Wymaga normalizacji cech\n",
        "- **Wolny na dużych zbiorach**: Wysoka złożoność obliczeniowa dla dużych zbiorów danych\n",
        "- **Wrażliwość na hiperparametry**: Wydajność zależy od odpowiedniego tuningu parametrów\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
