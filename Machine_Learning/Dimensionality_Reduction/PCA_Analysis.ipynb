{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Principal Component Analysis (PCA)**  \n",
        "### *Analiza głównych składowych*\n",
        "\n",
        "---\n",
        "\n",
        "## **English**\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving the most important information. It identifies the directions (principal components) along which the data varies the most.\n",
        "\n",
        "### **Key Concepts**\n",
        "\n",
        "1. **Principal Components**: Orthogonal directions that capture maximum variance\n",
        "2. **Eigenvalues**: Measure the amount of variance explained by each component\n",
        "3. **Eigenvectors**: Direction of the principal components\n",
        "4. **Explained Variance Ratio**: Proportion of dataset's variance along each component\n",
        "5. **Cumulative Explained Variance**: Total variance explained by first k components\n",
        "\n",
        "### **Mathematical Foundation**\n",
        "\n",
        "PCA finds principal components by solving the eigenvalue problem:\n",
        "$$\n",
        "C \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $C$ is the covariance matrix\n",
        "- $\\mathbf{v}$ are the eigenvectors (principal components)\n",
        "- $\\lambda$ are the eigenvalues (variance explained)\n",
        "\n",
        "The transformation is given by:\n",
        "$$\n",
        "\\mathbf{Z} = \\mathbf{X} \\mathbf{W}\n",
        "$$\n",
        "\n",
        "Where $\\mathbf{W}$ contains the first k principal components.\n",
        "\n",
        "### **Steps of PCA Algorithm**\n",
        "\n",
        "1. **Standardize the data** (if features have different scales)\n",
        "2. **Calculate covariance matrix** of the standardized data\n",
        "3. **Find eigenvalues and eigenvectors** of the covariance matrix\n",
        "4. **Sort eigenvalues** in descending order\n",
        "5. **Select top k eigenvectors** as principal components\n",
        "6. **Transform data** to the new k-dimensional space\n",
        "\n",
        "### **Applications**\n",
        "\n",
        "- **Dimensionality Reduction**: Reduce feature space while retaining information\n",
        "- **Data Visualization**: Visualize high-dimensional data in 2D/3D\n",
        "- **Noise Reduction**: Remove noise by keeping only important components\n",
        "- **Feature Engineering**: Create new features that capture most variance\n",
        "- **Data Compression**: Reduce storage requirements\n",
        "\n",
        "### **Advantages**\n",
        "- **Variance Preservation**: Maximizes variance retention\n",
        "- **Orthogonal Components**: No correlation between components\n",
        "- **Interpretability**: Clear understanding of data structure\n",
        "- **Computational Efficiency**: Faster training with reduced dimensions\n",
        "\n",
        "### **Disadvantages**\n",
        "- **Linear Transformation**: Cannot capture non-linear relationships\n",
        "- **Interpretability Loss**: Principal components may not have clear meaning\n",
        "- **Sensitivity to Scaling**: Results depend on feature scaling\n",
        "- **Information Loss**: Some variance is always lost\n",
        "\n",
        "---\n",
        "\n",
        "## **Polish**\n",
        "\n",
        "Analiza głównych składowych (PCA) to technika redukcji wymiarowości, która przekształca dane wielowymiarowe do przestrzeni o niższej wymiarowości, zachowując najważniejsze informacje. Identyfikuje kierunki (główne składowe), wzdłuż których dane najbardziej się różnią.\n",
        "\n",
        "### **Kluczowe pojęcia**\n",
        "\n",
        "1. **Główne składowe**: Ortogonalne kierunki, które wychwytują maksymalną wariancję\n",
        "2. **Wartości własne**: Mierzy ilość wariancji wyjaśnionej przez każdą składową\n",
        "3. **Wektory własne**: Kierunek głównych składowych\n",
        "4. **Współczynnik wyjaśnionej wariancji**: Proporcja wariancji zbioru danych wzdłuż każdej składowej\n",
        "5. **Skumulowana wyjaśniona wariancja**: Całkowita wariancja wyjaśniona przez pierwsze k składowych\n",
        "\n",
        "### **Podstawy matematyczne**\n",
        "\n",
        "PCA znajduje główne składowe rozwiązując problem wartości własnych:\n",
        "$$\n",
        "C \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "Gdzie:\n",
        "- $C$ to macierz kowariancji\n",
        "- $\\mathbf{v}$ to wektory własne (główne składowe)\n",
        "- $\\lambda$ to wartości własne (wyjaśniona wariancja)\n",
        "\n",
        "Transformacja dana jest przez:\n",
        "$$\n",
        "\\mathbf{Z} = \\mathbf{X} \\mathbf{W}\n",
        "$$\n",
        "\n",
        "Gdzie $\\mathbf{W}$ zawiera pierwsze k głównych składowych.\n",
        "\n",
        "### **Kroki algorytmu PCA**\n",
        "\n",
        "1. **Standaryzacja danych** (jeśli cechy mają różne skale)\n",
        "2. **Obliczenie macierzy kowariancji** standaryzowanych danych\n",
        "3. **Znalezienie wartości i wektorów własnych** macierzy kowariancji\n",
        "4. **Sortowanie wartości własnych** w porządku malejącym\n",
        "5. **Wybór pierwszych k wektorów własnych** jako głównych składowych\n",
        "6. **Transformacja danych** do nowej k-wymiarowej przestrzeni\n",
        "\n",
        "### **Zastosowania**\n",
        "\n",
        "- **Redukcja wymiarowości**: Zmniejszenie przestrzeni cech przy zachowaniu informacji\n",
        "- **Wizualizacja danych**: Wizualizacja wielowymiarowych danych w 2D/3D\n",
        "- **Redukcja szumu**: Usuwanie szumu przez zachowanie tylko ważnych składowych\n",
        "- **Inżynieria cech**: Tworzenie nowych cech, które wychwytują najwięcej wariancji\n",
        "- **Kompresja danych**: Zmniejszenie wymagań pamięciowych\n",
        "\n",
        "### **Zalety**\n",
        "- **Zachowanie wariancji**: Maksymalizuje retencję wariancji\n",
        "- **Ortogonalne składowe**: Brak korelacji między składowymi\n",
        "- **Interpretowalność**: Jasne zrozumienie struktury danych\n",
        "- **Efektywność obliczeniowa**: Szybsze trenowanie ze zmniejszonymi wymiarami\n",
        "\n",
        "### **Wady**\n",
        "- **Transformacja liniowa**: Nie może wychwycić relacji nieliniowych\n",
        "- **Utrata interpretowalności**: Główne składowe mogą nie mieć jasnego znaczenia\n",
        "- **Wrażliwość na skalowanie**: Wyniki zależą od skalowania cech\n",
        "- **Utrata informacji**: Pewna wariancja jest zawsze tracona\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PCA with Iris Dataset | PCA ze zbiorem danych Iris ===\n",
            "\n",
            "Original dataset shape | Kształt oryginalnego zbioru: (150, 4)\n",
            "Feature names | Nazwy cech: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Class names | Nazwy klas: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "Original feature means | Średnie oryginalne: [5.84333333 3.05733333 3.758      1.19933333]\n",
            "Scaled feature means | Średnie po skalowaniu: [-1.69031455e-15 -1.84297022e-15 -1.69864123e-15 -1.40924309e-15]\n",
            "Original feature std | Odchylenia oryginalne: [0.82530129 0.43441097 1.75940407 0.75969263]\n",
            "Scaled feature std | Odchylenia po skalowaniu: [1. 1. 1. 1.]\n",
            "\n",
            "Explained variance ratio | Współczynnik wyjaśnionej wariancji:\n",
            "  PC1: 0.7296 (72.96%)\n",
            "  PC2: 0.2285 (22.85%)\n",
            "  PC3: 0.0367 (3.67%)\n",
            "  PC4: 0.0052 (0.52%)\n",
            "\n",
            "Cumulative explained variance | Skumulowana wyjaśniona wariancja:\n",
            "  First 1 PCs: 0.7296 (72.96%)\n",
            "  First 2 PCs: 0.9581 (95.81%)\n",
            "  First 3 PCs: 0.9948 (99.48%)\n",
            "  First 4 PCs: 1.0000 (100.00%)\n",
            "\n",
            "Principal Components (Loadings) | Główne składowe:\n",
            "                     PC1    PC2    PC3    PC4\n",
            "sepal length (cm)  0.521  0.377 -0.720 -0.261\n",
            "sepal width (cm)  -0.269  0.923  0.244  0.124\n",
            "petal length (cm)  0.580  0.024  0.142  0.801\n",
            "petal width (cm)   0.565  0.067  0.634 -0.524\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris, load_digits, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Basic PCA Example with Iris Dataset\n",
        "# Podstawowy przykład PCA ze zbiorem danych Iris\n",
        "print(\"=== PCA with Iris Dataset | PCA ze zbiorem danych Iris ===\\n\")\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "\n",
        "print(f\"Original dataset shape | Kształt oryginalnego zbioru: {X_iris.shape}\")\n",
        "print(f\"Feature names | Nazwy cech: {iris.feature_names}\")\n",
        "print(f\"Class names | Nazwy klas: {iris.target_names}\")\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_iris_scaled = scaler.fit_transform(X_iris)\n",
        "\n",
        "print(f\"\\nOriginal feature means | Średnie oryginalne: {X_iris.mean(axis=0)}\")\n",
        "print(f\"Scaled feature means | Średnie po skalowaniu: {X_iris_scaled.mean(axis=0)}\")\n",
        "print(f\"Original feature std | Odchylenia oryginalne: {X_iris.std(axis=0)}\")\n",
        "print(f\"Scaled feature std | Odchylenia po skalowaniu: {X_iris_scaled.std(axis=0)}\")\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_iris_pca = pca.fit_transform(X_iris_scaled)\n",
        "\n",
        "# Print explained variance\n",
        "print(f\"\\nExplained variance ratio | Współczynnik wyjaśnionej wariancji:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nCumulative explained variance | Skumulowana wyjaśniona wariancja:\")\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "for i, cum_ratio in enumerate(cumsum):\n",
        "    print(f\"  First {i+1} PCs: {cum_ratio:.4f} ({cum_ratio*100:.2f}%)\")\n",
        "\n",
        "# Components (loadings)\n",
        "print(f\"\\nPrincipal Components (Loadings) | Główne składowe:\")\n",
        "components_df = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    columns=[f'PC{i+1}' for i in range(len(pca.components_))],\n",
        "    index=iris.feature_names\n",
        ")\n",
        "print(components_df.round(3))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
