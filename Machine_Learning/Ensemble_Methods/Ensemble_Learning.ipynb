{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Ensemble Learning and Advanced Methods**  \n",
        "### *Uczenie zespołowe i zaawansowane metody*\n",
        "\n",
        "---\n",
        "\n",
        "## **English**\n",
        "\n",
        "Ensemble Learning combines multiple machine learning models to create a stronger predictor than any individual model alone. The key idea is that a group of weak learners can come together to form a strong learner. This approach often leads to better predictive performance and increased robustness.\n",
        "\n",
        "### **Key Concepts**\n",
        "\n",
        "1. **Weak Learners**: Simple models that perform slightly better than random guessing\n",
        "2. **Strong Learner**: A model that can achieve arbitrarily high accuracy\n",
        "3. **Diversity**: Different models should make different types of errors\n",
        "4. **Aggregation**: Combining predictions from multiple models\n",
        "5. **Bias-Variance Trade-off**: Ensembles can reduce both bias and variance\n",
        "\n",
        "### **Main Ensemble Methods**\n",
        "\n",
        "#### **1. Bagging (Bootstrap Aggregating)**\n",
        "- **Principle**: Train multiple models on different bootstrap samples\n",
        "- **Aggregation**: Voting (classification) or averaging (regression)\n",
        "- **Examples**: Random Forest, Extra Trees\n",
        "- **Reduces**: Variance and overfitting\n",
        "\n",
        "#### **2. Boosting**\n",
        "- **Principle**: Sequential training where each model corrects previous errors\n",
        "- **Aggregation**: Weighted combination of weak learners\n",
        "- **Examples**: AdaBoost, Gradient Boosting, XGBoost\n",
        "- **Reduces**: Bias and underfitting\n",
        "\n",
        "#### **3. Stacking (Stacked Generalization)**\n",
        "- **Principle**: Use a meta-learner to combine base model predictions\n",
        "- **Aggregation**: Meta-model learns optimal combination\n",
        "- **Examples**: Stacked ensembles with various base models\n",
        "- **Reduces**: Both bias and variance\n",
        "\n",
        "#### **4. Voting**\n",
        "- **Hard Voting**: Majority vote for classification\n",
        "- **Soft Voting**: Average predicted probabilities\n",
        "- **Simple but effective**: Works well with diverse models\n",
        "\n",
        "### **Advanced Ensemble Techniques**\n",
        "\n",
        "#### **Gradient Boosting**\n",
        "Sequential ensemble where each model fits the residuals of the previous models:\n",
        "$$\n",
        "F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n",
        "$$\n",
        "\n",
        "Where $h_m(x)$ is trained on the residuals of $F_{m-1}(x)$.\n",
        "\n",
        "#### **AdaBoost**\n",
        "Adaptive boosting that adjusts weights based on classification errors:\n",
        "$$\n",
        "\\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1-\\epsilon_m}{\\epsilon_m}\\right)\n",
        "$$\n",
        "\n",
        "Where $\\epsilon_m$ is the weighted error rate of classifier $m$.\n",
        "\n",
        "### **Advantages**\n",
        "- **Better Performance**: Often outperforms individual models\n",
        "- **Robustness**: Less sensitive to outliers and noise\n",
        "- **Generalization**: Reduces overfitting\n",
        "- **Flexibility**: Can combine different types of models\n",
        "\n",
        "### **Disadvantages**\n",
        "- **Complexity**: More difficult to interpret\n",
        "- **Computational Cost**: Requires training multiple models\n",
        "- **Memory Usage**: Need to store multiple models\n",
        "- **Hyperparameter Tuning**: More parameters to optimize\n",
        "\n",
        "---\n",
        "\n",
        "## **Polish**\n",
        "\n",
        "Uczenie zespołowe łączy wiele modeli uczenia maszynowego, aby stworzyć silniejszy predyktor niż jakikolwiek pojedynczy model. Kluczową ideą jest to, że grupa słabych uczniów może połączyć się, tworząc silnego ucznia. To podejście często prowadzi do lepszej wydajności predykcyjnej i zwiększonej odporności.\n",
        "\n",
        "### **Kluczowe pojęcia**\n",
        "\n",
        "1. **Słabi uczniowie**: Proste modele, które działają nieco lepiej niż losowe zgadywanie\n",
        "2. **Silny uczeń**: Model, który może osiągnąć arbitralnie wysoką dokładność\n",
        "3. **Różnorodność**: Różne modele powinny popełniać różne typy błędów\n",
        "4. **Agregacja**: Łączenie przewidywań z wielu modeli\n",
        "5. **Kompromis bias-wariancja**: Zespoły mogą zmniejszyć zarówno bias, jak i wariancję\n",
        "\n",
        "### **Główne metody zespołowe**\n",
        "\n",
        "#### **1. Bagging (Bootstrap Aggregating)**\n",
        "- **Zasada**: Trenowanie wielu modeli na różnych próbkach bootstrap\n",
        "- **Agregacja**: Głosowanie (klasyfikacja) lub uśrednianie (regresja)\n",
        "- **Przykłady**: Random Forest, Extra Trees\n",
        "- **Zmniejsza**: Wariancję i przeuczenie\n",
        "\n",
        "#### **2. Boosting**\n",
        "- **Zasada**: Sekwencyjne trenowanie, gdzie każdy model koryguje poprzednie błędy\n",
        "- **Agregacja**: Ważona kombinacja słabych uczniów\n",
        "- **Przykłady**: AdaBoost, Gradient Boosting, XGBoost\n",
        "- **Zmniejsza**: Bias i niedouczenie\n",
        "\n",
        "#### **3. Stacking (Stacked Generalization)**\n",
        "- **Zasada**: Użycie meta-ucznia do łączenia przewidywań modeli bazowych\n",
        "- **Agregacja**: Meta-model uczy się optymalnej kombinacji\n",
        "- **Przykłady**: Zespoły układane z różnymi modelami bazowymi\n",
        "- **Zmniejsza**: Zarówno bias, jak i wariancję\n",
        "\n",
        "#### **4. Voting**\n",
        "- **Głosowanie twarde**: Głosowanie większości dla klasyfikacji\n",
        "- **Głosowanie miękkie**: Uśrednianie przewidywanych prawdopodobieństw\n",
        "- **Proste, ale skuteczne**: Działa dobrze z różnorodnymi modelami\n",
        "\n",
        "### **Zaawansowane techniki zespołowe**\n",
        "\n",
        "#### **Gradient Boosting**\n",
        "Sekwencyjny zespół, gdzie każdy model dopasowuje reszty poprzednich modeli:\n",
        "$$\n",
        "F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n",
        "$$\n",
        "\n",
        "Gdzie $h_m(x)$ jest trenowany na resztach $F_{m-1}(x)$.\n",
        "\n",
        "#### **AdaBoost**\n",
        "Adaptacyjne wzmacnianie, które dostosowuje wagi na podstawie błędów klasyfikacji:\n",
        "$$\n",
        "\\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1-\\epsilon_m}{\\epsilon_m}\\right)\n",
        "$$\n",
        "\n",
        "Gdzie $\\epsilon_m$ to ważony współczynnik błędów klasyfikatora $m$.\n",
        "\n",
        "### **Zalety**\n",
        "- **Lepsza wydajność**: Często przewyższa pojedyncze modele\n",
        "- **Odporność**: Mniej wrażliwy na wartości odstające i szum\n",
        "- **Generalizacja**: Zmniejsza przeuczenie\n",
        "- **Elastyczność**: Może łączyć różne typy modeli\n",
        "\n",
        "### **Wady**\n",
        "- **Złożoność**: Trudniejsze do interpretacji\n",
        "- **Koszt obliczeniowy**: Wymaga trenowania wielu modeli\n",
        "- **Zużycie pamięci**: Konieczność przechowywania wielu modeli\n",
        "- **Tuning hiperparametrów**: Więcej parametrów do optymalizacji\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
