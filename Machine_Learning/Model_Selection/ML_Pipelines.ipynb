{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Machine Learning Pipelines**\n",
        "## **Pipeline uczenia maszynowego**\n",
        "\n",
        "---\n",
        "\n",
        "### **Overview / Przegląd**\n",
        "\n",
        "ML Pipelines provide a clean and efficient way to organize machine learning workflows. They chain together multiple processing steps, ensuring that data transformations are applied consistently during training and prediction.\n",
        "\n",
        "Pipeline ML zapewniają czysty i efektywny sposób organizacji przepływów pracy uczenia maszynowego. Łączą one w łańcuch wiele kroków przetwarzania, zapewniając, że transformacje danych są stosowane konsekwentnie podczas treningu i przewidywania.\n",
        "\n",
        "### **Benefits / Korzyści**\n",
        "\n",
        "1. **Reproducibility** - Consistent data processing / Spójna przetwarzanie danych\n",
        "2. **Code Organization** - Clean and maintainable code / Czysty i łatwy w utrzymaniu kod\n",
        "3. **Prevent Data Leakage** - Proper train/test separation / Właściwe rozdzielenie train/test\n",
        "4. **Hyperparameter Tuning** - Easy to tune entire pipeline / Łatwe strojenie całego pipeline\n",
        "5. **Production Ready** - Easy deployment / Łatwe wdrożenie\n",
        "\n",
        "### **Pipeline Components / Komponenty Pipeline**\n",
        "\n",
        "- **Transformers**: Modify data (scaling, encoding, etc.) / Modyfikują dane (skalowanie, kodowanie, itp.)\n",
        "- **Estimators**: Learn from data (classifiers, regressors) / Uczą się z danych (klasyfikatory, regresory)\n",
        "- **Pipeline**: Chains transformers and estimators / Łączy transformery i estymatory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries / Importowanie wymaganych bibliotek\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_boston, make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots / Ustawienie stylu wykresów\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully / Biblioteki zaimportowane pomyślnie\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CHANGE THE DATASET! Removed in >1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a complex dataset / Tworzenie złożonego zbioru danych\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate classification dataset with different feature types\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Add some categorical features (simulated) / Dodanie kilku cech kategorycznych (symulowanych)\n",
        "categorical_features = np.random.randint(0, 3, size=(1000, 3))\n",
        "X_with_categorical = np.hstack([X, categorical_features])\n",
        "\n",
        "# Create feature names / Tworzenie nazw cech\n",
        "feature_names = [f'numeric_{i}' for i in range(20)] + [f'categorical_{i}' for i in range(3)]\n",
        "\n",
        "# Create DataFrame / Tworzenie DataFrame\n",
        "df = pd.DataFrame(X_with_categorical, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(f\"Dataset shape / Kształt zbioru danych: {df.shape}\")\n",
        "print(f\"Feature types / Typy cech:\")\n",
        "print(f\"- Numeric features / Cechy numeryczne: {len([col for col in df.columns if 'numeric' in col])}\")\n",
        "print(f\"- Categorical features / Cechy kategoryczne: {len([col for col in df.columns if 'categorical' in col])}\")\n",
        "print(f\"\\\\nClass distribution / Rozkład klas:\")\n",
        "print(df['target'].value_counts())\n",
        "\n",
        "# Display first few rows / Wyświetlenie pierwszych kilku wierszy\n",
        "print(\"\\\\nFirst 5 rows / Pierwsze 5 wierszy:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data / Przygotowanie danych\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split numeric and categorical features / Podział na cechy numeryczne i kategoryczne\n",
        "numeric_features = [col for col in X.columns if 'numeric' in col]\n",
        "categorical_features = [col for col in X.columns if 'categorical' in col]\n",
        "\n",
        "print(f\"Numeric features / Cechy numeryczne: {len(numeric_features)}\")\n",
        "print(f\"Categorical features / Cechy kategoryczne: {len(categorical_features)}\")\n",
        "\n",
        "# Split the data / Podział danych\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1. SIMPLE PIPELINE / PROSTY PIPELINE\n",
        "print(\"\\\\n=== 1. SIMPLE PIPELINE ===\")\n",
        "\n",
        "# Create a simple pipeline / Tworzenie prostego pipeline\n",
        "simple_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(random_state=42))\n",
        "])\n",
        "\n",
        "# Train and evaluate / Trenowanie i ocena\n",
        "simple_pipeline.fit(X_train[numeric_features], y_train)\n",
        "y_pred_simple = simple_pipeline.predict(X_test[numeric_features])\n",
        "simple_accuracy = accuracy_score(y_test, y_pred_simple)\n",
        "\n",
        "print(f\"Simple Pipeline Accuracy / Dokładność prostego pipeline: {simple_accuracy:.4f}\")\n",
        "\n",
        "# 2. COMPLEX PIPELINE WITH FEATURE ENGINEERING / ZŁOŻONY PIPELINE Z INŻYNIERIĄ CECH\n",
        "print(\"\\\\n=== 2. COMPLEX PIPELINE WITH FEATURE ENGINEERING ===\")\n",
        "\n",
        "# Create complex pipeline / Tworzenie złożonego pipeline\n",
        "complex_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('feature_selection', SelectKBest(f_classif, k=50)),\n",
        "    ('pca', PCA(n_components=0.95)),  # Keep 95% of variance\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Train and evaluate / Trenowanie i ocena\n",
        "complex_pipeline.fit(X_train[numeric_features], y_train)\n",
        "y_pred_complex = complex_pipeline.predict(X_test[numeric_features])\n",
        "complex_accuracy = accuracy_score(y_test, y_pred_complex)\n",
        "\n",
        "print(f\"Complex Pipeline Accuracy / Dokładność złożonego pipeline: {complex_accuracy:.4f}\")\n",
        "\n",
        "# Show pipeline steps / Pokazanie kroków pipeline\n",
        "print(\"\\\\nPipeline steps / Kroki pipeline:\")\n",
        "for i, (name, transformer) in enumerate(complex_pipeline.steps):\n",
        "    print(f\"{i+1}. {name}: {type(transformer).__name__}\")\n",
        "\n",
        "# 3. COLUMN TRANSFORMER PIPELINE / PIPELINE Z COLUMN TRANSFORMER\n",
        "print(\"\\\\n=== 3. COLUMN TRANSFORMER PIPELINE ===\")\n",
        "\n",
        "# Create preprocessing for different column types / Tworzenie preprocessingu dla różnych typów kolumn\n",
        "numeric_transformer = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    ('onehot', pd.get_dummies)  # Simple one-hot encoding simulation\n",
        "])\n",
        "\n",
        "# Create column transformer / Tworzenie column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        # For categorical, we'll handle separately due to pandas integration\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create full pipeline / Tworzenie pełnego pipeline\n",
        "column_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectKBest(f_classif, k=15)),\n",
        "    ('classifier', SVC(kernel='rbf', random_state=42))\n",
        "])\n",
        "\n",
        "# Train and evaluate / Trenowanie i ocena\n",
        "column_pipeline.fit(X_train, y_train)\n",
        "y_pred_column = column_pipeline.predict(X_test)\n",
        "column_accuracy = accuracy_score(y_test, y_pred_column)\n",
        "\n",
        "print(f\"Column Transformer Pipeline Accuracy / Dokładność pipeline z Column Transformer: {column_accuracy:.4f}\")\n",
        "\n",
        "# Compare all pipelines / Porównanie wszystkich pipeline\n",
        "results_comparison = pd.DataFrame({\n",
        "    'Pipeline': ['Simple', 'Complex', 'Column Transformer'],\n",
        "    'Accuracy': [simple_accuracy, complex_accuracy, column_accuracy],\n",
        "    'Features_Used': [len(numeric_features), 'Engineered', 'All']\n",
        "})\n",
        "\n",
        "print(\"\\\\n=== PIPELINE COMPARISON / PORÓWNANIE PIPELINE ===\")\n",
        "print(results_comparison.to_string(index=False, float_format='%.4f'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. HYPERPARAMETER TUNING WITH PIPELINES / STROJENIE HIPERPARAMETRÓW Z PIPELINE\n",
        "print(\"\\\\n=== 4. HYPERPARAMETER TUNING WITH PIPELINES ===\")\n",
        "\n",
        "# Define pipeline for tuning / Definicja pipeline do strojenia\n",
        "tuning_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('feature_selection', SelectKBest()),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define parameter grid / Definicja siatki parametrów\n",
        "param_grid = {\n",
        "    'feature_selection__k': [10, 15, 20],\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [3, 5, None],\n",
        "    'classifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Perform grid search / Wykonanie grid search\n",
        "grid_search = GridSearchCV(\n",
        "    tuning_pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train[numeric_features], y_train)\n",
        "\n",
        "print(f\"\\\\nBest parameters / Najlepsze parametry: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score / Najlepszy wynik CV: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Test performance / Wydajność testowa\n",
        "y_pred_tuned = grid_search.predict(X_test[numeric_features])\n",
        "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"Tuned Pipeline Test Accuracy / Dokładność testowa dostrojonego pipeline: {tuned_accuracy:.4f}\")\n",
        "\n",
        "# 5. VISUALIZATION OF PIPELINE RESULTS / WIZUALIZACJA WYNIKÓW PIPELINE\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Accuracy comparison / Porównanie dokładności\n",
        "pipeline_names = ['Simple', 'Complex', 'Column\\\\nTransformer', 'Tuned']\n",
        "accuracies = [simple_accuracy, complex_accuracy, column_accuracy, tuned_accuracy]\n",
        "\n",
        "axes[0, 0].bar(pipeline_names, accuracies, alpha=0.7, color=['blue', 'green', 'orange', 'red'])\n",
        "axes[0, 0].set_title('Pipeline Accuracy Comparison\\\\nPorównanie dokładności pipeline')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_ylim(0.8, 1.0)\n",
        "for i, acc in enumerate(accuracies):\n",
        "    axes[0, 0].text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 2. Feature importance from best model / Ważność cech z najlepszego modelu\n",
        "best_model = grid_search.best_estimator_\n",
        "feature_selector = best_model.named_steps['feature_selection']\n",
        "classifier = best_model.named_steps['classifier']\n",
        "\n",
        "# Get selected features / Otrzymanie wybranych cech\n",
        "selected_features = feature_selector.get_support()\n",
        "selected_feature_names = [numeric_features[i] for i, selected in enumerate(selected_features) if selected]\n",
        "\n",
        "# Get feature importances / Otrzymanie ważności cech\n",
        "feature_importances = classifier.feature_importances_\n",
        "\n",
        "# Plot feature importances / Wykres ważności cech\n",
        "top_features = sorted(zip(selected_feature_names, feature_importances), \n",
        "                     key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "feature_names_plot = [name for name, _ in top_features]\n",
        "importance_values = [importance for _, importance in top_features]\n",
        "\n",
        "axes[0, 1].barh(range(len(feature_names_plot)), importance_values, alpha=0.7)\n",
        "axes[0, 1].set_yticks(range(len(feature_names_plot)))\n",
        "axes[0, 1].set_yticklabels(feature_names_plot)\n",
        "axes[0, 1].set_title('Top 10 Feature Importances\\\\nNajważniejsze 10 cech')\n",
        "axes[0, 1].set_xlabel('Importance')\n",
        "\n",
        "# 3. Cross-validation scores / Wyniki walidacji krzyżowej\n",
        "cv_scores = cross_val_score(grid_search.best_estimator_, \n",
        "                           X_train[numeric_features], y_train, cv=5)\n",
        "\n",
        "axes[1, 0].boxplot([cv_scores], labels=['CV Scores'])\n",
        "axes[1, 0].scatter([1] * len(cv_scores), cv_scores, alpha=0.7, color='red')\n",
        "axes[1, 0].set_title('Cross-Validation Scores\\\\nWyniki walidacji krzyżowej')\n",
        "axes[1, 0].set_ylabel('Accuracy')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add mean line / Dodanie linii średniej\n",
        "mean_cv = cv_scores.mean()\n",
        "axes[1, 0].axhline(y=mean_cv, color='green', linestyle='--', \n",
        "                  label=f'Mean: {mean_cv:.3f}')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 4. Pipeline complexity comparison / Porównanie złożoności pipeline\n",
        "pipeline_complexity = {\n",
        "    'Simple': 2,  # scaler + classifier\n",
        "    'Complex': 5,  # scaler + poly + selection + pca + classifier\n",
        "    'Column Transformer': 3,  # preprocessor + selection + classifier\n",
        "    'Tuned': 3   # scaler + selection + classifier\n",
        "}\n",
        "\n",
        "complexity_names = list(pipeline_complexity.keys())\n",
        "complexity_values = list(pipeline_complexity.values())\n",
        "\n",
        "bars = axes[1, 1].bar(complexity_names, complexity_values, alpha=0.7)\n",
        "axes[1, 1].set_title('Pipeline Complexity\\\\nZłożoność pipeline')\n",
        "axes[1, 1].set_ylabel('Number of Steps')\n",
        "\n",
        "# Color bars by accuracy / Kolorowanie słupków według dokładności\n",
        "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
        "    bar.set_color(plt.cm.viridis(acc))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final summary / Końcowe podsumowanie\n",
        "print(\"\\\\n=== FINAL SUMMARY / KOŃCOWE PODSUMOWANIE ===\")\n",
        "final_results = pd.DataFrame({\n",
        "    'Pipeline': pipeline_names,\n",
        "    'Accuracy': accuracies,\n",
        "    'Complexity': [pipeline_complexity[name] for name in pipeline_names]\n",
        "})\n",
        "\n",
        "print(final_results.to_string(index=False, float_format='%.4f'))\n",
        "print(f\"\\\\nBest performing pipeline / Najlepiej działający pipeline: {pipeline_names[np.argmax(accuracies)]}\")\n",
        "print(f\"Best accuracy / Najlepsza dokładność: {max(accuracies):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Machine Learning Pipelines**  \n",
        "### *Pipeline'y uczenia maszynowego*\n",
        "\n",
        "---\n",
        "\n",
        "## **English**\n",
        "\n",
        "Machine Learning Pipelines are a sequence of data processing steps and model training/prediction steps chained together. They provide a clean, reproducible way to apply the same sequence of transformations to both training and test data, preventing data leakage and ensuring consistent preprocessing.\n",
        "\n",
        "### **Key Concepts**\n",
        "\n",
        "1. **Pipeline**: Sequential chain of data transformations and estimators\n",
        "2. **Transformers**: Objects that transform data (scaling, encoding, feature selection)\n",
        "3. **Estimators**: Objects that learn from data (classifiers, regressors)\n",
        "4. **Fit-Transform**: Two-step process for learning and applying transformations\n",
        "5. **Data Leakage Prevention**: Ensures proper separation of training and test data\n",
        "\n",
        "### **Benefits of Pipelines**\n",
        "\n",
        "#### **1. Reproducibility**\n",
        "- Consistent preprocessing across train/validation/test sets\n",
        "- Eliminates manual step-by-step transformations\n",
        "- Ensures same transformations in production\n",
        "\n",
        "#### **2. Data Leakage Prevention**\n",
        "- Prevents using test data statistics in preprocessing\n",
        "- Proper cross-validation with preprocessing\n",
        "- Maintains temporal order in time series\n",
        "\n",
        "#### **3. Code Organization**\n",
        "- Clean, readable code structure\n",
        "- Modular components\n",
        "- Easy to modify and extend\n",
        "\n",
        "#### **4. Hyperparameter Tuning**\n",
        "- Tune preprocessing and model parameters together\n",
        "- Grid search across entire pipeline\n",
        "- Nested parameter optimization\n",
        "\n",
        "### **Pipeline Components**\n",
        "\n",
        "#### **Transformers (Preprocessing)**\n",
        "- **StandardScaler**: Standardize features (mean=0, std=1)\n",
        "- **MinMaxScaler**: Scale features to [0,1] range\n",
        "- **RobustScaler**: Scale using median and IQR\n",
        "- **OneHotEncoder**: Convert categorical to binary features\n",
        "- **LabelEncoder**: Convert categorical to numeric labels\n",
        "- **PCA**: Dimensionality reduction\n",
        "- **SelectKBest**: Feature selection\n",
        "- **PolynomialFeatures**: Generate polynomial features\n",
        "\n",
        "#### **Estimators (Models)**\n",
        "- **Classification**: LogisticRegression, RandomForest, SVM\n",
        "- **Regression**: LinearRegression, Ridge, Lasso\n",
        "- **Clustering**: KMeans, DBSCAN\n",
        "- **Dimensionality Reduction**: PCA, t-SNE\n",
        "\n",
        "### **Pipeline Creation**\n",
        "\n",
        "#### **Method 1: Pipeline Class**\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "```\n",
        "\n",
        "#### **Method 2: make_pipeline Function**\n",
        "```python\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    RandomForestClassifier()\n",
        ")\n",
        "```\n",
        "\n",
        "### **ColumnTransformer**\n",
        "\n",
        "For handling different transformations on different columns:\n",
        "\n",
        "```python\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(), categorical_features)\n",
        "])\n",
        "```\n",
        "\n",
        "### **Pipeline Operations**\n",
        "\n",
        "#### **Training**\n",
        "```python\n",
        "# Fit the entire pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "```\n",
        "\n",
        "#### **Accessing Components**\n",
        "```python\n",
        "# Access specific steps\n",
        "scaler = pipeline.named_steps['scaler']\n",
        "classifier = pipeline.named_steps['classifier']\n",
        "\n",
        "# Get transformed data\n",
        "X_scaled = pipeline[:-1].transform(X_test)\n",
        "```\n",
        "\n",
        "### **Hyperparameter Tuning with Pipelines**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'scaler__with_mean': [True, False],\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [3, 5, None]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "### **Advanced Pipeline Patterns**\n",
        "\n",
        "#### **1. Feature Union**\n",
        "Combine multiple feature extraction methods:\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "feature_union = FeatureUnion([\n",
        "    ('pca', PCA(n_components=2)),\n",
        "    ('select_k_best', SelectKBest(k=3))\n",
        "])\n",
        "```\n",
        "\n",
        "#### **2. Custom Transformers**\n",
        "Create custom preprocessing steps:\n",
        "\n",
        "```python\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        # Custom transformation logic\n",
        "        return X_transformed\n",
        "```\n",
        "\n",
        "#### **3. Conditional Processing**\n",
        "Different processing based on data characteristics:\n",
        "\n",
        "```python\n",
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "numeric_selector = make_column_selector(dtype_include='number')\n",
        "categorical_selector = make_column_selector(dtype_include='object')\n",
        "```\n",
        "\n",
        "### **Best Practices**\n",
        "\n",
        "#### **1. Design Principles**\n",
        "- Keep transformations simple and interpretable\n",
        "- Use meaningful names for pipeline steps\n",
        "- Document transformation logic\n",
        "- Test each component separately\n",
        "\n",
        "#### **2. Performance Optimization**\n",
        "- Use `memory` parameter for caching\n",
        "- Leverage `n_jobs` for parallel processing\n",
        "- Consider sparse matrices for large datasets\n",
        "- Profile pipeline performance\n",
        "\n",
        "#### **3. Error Handling**\n",
        "- Validate input data types\n",
        "- Handle missing values appropriately\n",
        "- Check for data leakage\n",
        "- Monitor pipeline performance in production\n",
        "\n",
        "### **Common Pitfalls**\n",
        "\n",
        "1. **Data Leakage**: Fitting transformers on entire dataset\n",
        "2. **Inconsistent Preprocessing**: Different steps for train/test\n",
        "3. **Order Dependency**: Wrong order of transformations\n",
        "4. **Memory Issues**: Not using sparse matrices when appropriate\n",
        "5. **Overfitting**: Too complex preprocessing steps\n",
        "\n",
        "### **Production Deployment**\n",
        "\n",
        "#### **Model Serialization**\n",
        "```python\n",
        "import joblib\n",
        "\n",
        "# Save pipeline\n",
        "joblib.dump(pipeline, 'model_pipeline.pkl')\n",
        "\n",
        "# Load pipeline\n",
        "pipeline = joblib.load('model_pipeline.pkl')\n",
        "```\n",
        "\n",
        "#### **Version Control**\n",
        "- Track pipeline versions\n",
        "- Document preprocessing steps\n",
        "- Maintain backward compatibility\n",
        "- Monitor data drift\n",
        "\n",
        "---\n",
        "\n",
        "## **Polish**\n",
        "\n",
        "Pipeline'y uczenia maszynowego to sekwencja kroków przetwarzania danych i kroków treningu/przewidywania modelu połączonych razem. Zapewniają czysty, reprodukowalny sposób stosowania tej samej sekwencji transformacji zarówno do danych treningowych, jak i testowych, zapobiegając wyciekom danych i zapewniając spójne przetwarzanie wstępne.\n",
        "\n",
        "### **Kluczowe pojęcia**\n",
        "\n",
        "1. **Pipeline**: Sekwencyjna łańcuch transformacji danych i estymatorów\n",
        "2. **Transformatory**: Obiekty transformujące dane (skalowanie, kodowanie, selekcja cech)\n",
        "3. **Estymatory**: Obiekty uczące się z danych (klasyfikatory, regresory)\n",
        "4. **Fit-Transform**: Dwuetapowy proces uczenia i stosowania transformacji\n",
        "5. **Zapobieganie wyciekom danych**: Zapewnia właściwe oddzielenie danych treningowych i testowych\n",
        "\n",
        "### **Korzyści z pipeline'ów**\n",
        "\n",
        "#### **1. Reprodukowalność**\n",
        "- Spójne przetwarzanie wstępne w zbiorach train/validation/test\n",
        "- Eliminuje manualne transformacje krok po kroku\n",
        "- Zapewnia te same transformacje w produkcji\n",
        "\n",
        "#### **2. Zapobieganie wyciekom danych**\n",
        "- Zapobiega używaniu statystyk danych testowych w preprocessing\n",
        "- Właściwa walidacja krzyżowa z przetwarzaniem wstępnym\n",
        "- Utrzymuje porządek czasowy w szeregach czasowych\n",
        "\n",
        "#### **3. Organizacja kodu**\n",
        "- Czysta, czytelna struktura kodu\n",
        "- Komponenty modularne\n",
        "- Łatwe do modyfikacji i rozszerzania\n",
        "\n",
        "#### **4. Strojenie hiperparametrów**\n",
        "- Strojenie parametrów preprocessing i modelu razem\n",
        "- Grid search przez cały pipeline\n",
        "- Zagnieżdżona optymalizacja parametrów\n",
        "\n",
        "### **Komponenty pipeline'a**\n",
        "\n",
        "#### **Transformatory (Preprocessing)**\n",
        "- **StandardScaler**: Standaryzacja cech (mean=0, std=1)\n",
        "- **MinMaxScaler**: Skalowanie cech do zakresu [0,1]\n",
        "- **RobustScaler**: Skalowanie używając mediany i IQR\n",
        "- **OneHotEncoder**: Konwersja kategorycznych na binarne cechy\n",
        "- **LabelEncoder**: Konwersja kategorycznych na etykiety numeryczne\n",
        "- **PCA**: Redukcja wymiarowości\n",
        "- **SelectKBest**: Selekcja cech\n",
        "- **PolynomialFeatures**: Generowanie cech wielomianowych\n",
        "\n",
        "#### **Estymatory (Modele)**\n",
        "- **Klasyfikacja**: LogisticRegression, RandomForest, SVM\n",
        "- **Regresja**: LinearRegression, Ridge, Lasso\n",
        "- **Klasteryzacja**: KMeans, DBSCAN\n",
        "- **Redukcja wymiarowości**: PCA, t-SNE\n",
        "\n",
        "### **Tworzenie pipeline'a**\n",
        "\n",
        "#### **Metoda 1: Klasa Pipeline**\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "```\n",
        "\n",
        "#### **Metoda 2: Funkcja make_pipeline**\n",
        "```python\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    RandomForestClassifier()\n",
        ")\n",
        "```\n",
        "\n",
        "### **ColumnTransformer**\n",
        "\n",
        "Do obsługi różnych transformacji na różnych kolumnach:\n",
        "\n",
        "```python\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(), categorical_features)\n",
        "])\n",
        "```\n",
        "\n",
        "### **Operacje pipeline'a**\n",
        "\n",
        "#### **Trenowanie**\n",
        "```python\n",
        "# Dopasowanie całego pipeline'a\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Przewidywania\n",
        "y_pred = pipeline.predict(X_test)\n",
        "```\n",
        "\n",
        "#### **Dostęp do komponentów**\n",
        "```python\n",
        "# Dostęp do konkretnych kroków\n",
        "scaler = pipeline.named_steps['scaler']\n",
        "classifier = pipeline.named_steps['classifier']\n",
        "\n",
        "# Otrzymanie przekształconych danych\n",
        "X_scaled = pipeline[:-1].transform(X_test)\n",
        "```\n",
        "\n",
        "### **Strojenie hiperparametrów z pipeline'ami**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'scaler__with_mean': [True, False],\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [3, 5, None]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "### **Zaawansowane wzorce pipeline'ów**\n",
        "\n",
        "#### **1. Feature Union**\n",
        "Łączenie wielu metod ekstrakcji cech:\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "feature_union = FeatureUnion([\n",
        "    ('pca', PCA(n_components=2)),\n",
        "    ('select_k_best', SelectKBest(k=3))\n",
        "])\n",
        "```\n",
        "\n",
        "#### **2. Transformatory niestandardowe**\n",
        "Tworzenie niestandardowych kroków preprocessing:\n",
        "\n",
        "```python\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        # Logika niestandardowej transformacji\n",
        "        return X_transformed\n",
        "```\n",
        "\n",
        "#### **3. Przetwarzanie warunkowe**\n",
        "Różne przetwarzanie w oparciu o charakterystyki danych:\n",
        "\n",
        "```python\n",
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "numeric_selector = make_column_selector(dtype_include='number')\n",
        "categorical_selector = make_column_selector(dtype_include='object')\n",
        "```\n",
        "\n",
        "### **Najlepsze praktyki**\n",
        "\n",
        "#### **1. Zasady projektowania**\n",
        "- Utrzymuj transformacje proste i interpretowalnie\n",
        "- Używaj znaczących nazw dla kroków pipeline'a\n",
        "- Dokumentuj logikę transformacji\n",
        "- Testuj każdy komponent osobno\n",
        "\n",
        "#### **2. Optymalizacja wydajności**\n",
        "- Używaj parametru `memory` do cache'owania\n",
        "- Wykorzystuj `n_jobs` do przetwarzania równoległego\n",
        "- Rozważ macierze rzadkie dla dużych zbiorów danych\n",
        "- Profiluj wydajność pipeline'a\n",
        "\n",
        "#### **3. Obsługa błędów**\n",
        "- Waliduj typy danych wejściowych\n",
        "- Odpowiednio obsługuj brakujące wartości\n",
        "- Sprawdzaj wycieki danych\n",
        "- Monitoruj wydajność pipeline'a w produkcji\n",
        "\n",
        "### **Częste pułapki**\n",
        "\n",
        "1. **Wyciek danych**: Dopasowywanie transformatorów na całym zbiorze danych\n",
        "2. **Niespójne preprocessing**: Różne kroki dla train/test\n",
        "3. **Zależność od kolejności**: Zła kolejność transformacji\n",
        "4. **Problemy z pamięcią**: Nieużywanie macierzy rzadkich gdy to odpowiednie\n",
        "5. **Przeuczenie**: Zbyt złożone kroki preprocessing\n",
        "\n",
        "### **Wdrożenie produkcyjne**\n",
        "\n",
        "#### **Serializacja modelu**\n",
        "```python\n",
        "import joblib\n",
        "\n",
        "# Zapisz pipeline\n",
        "joblib.dump(pipeline, 'model_pipeline.pkl')\n",
        "\n",
        "# Wczytaj pipeline\n",
        "pipeline = joblib.load('model_pipeline.pkl')\n",
        "```\n",
        "\n",
        "#### **Kontrola wersji**\n",
        "- Śledź wersje pipeline'ów\n",
        "- Dokumentuj kroki preprocessing\n",
        "- Utrzymuj kompatybilność wsteczną\n",
        "- Monitoruj dryft danych\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
