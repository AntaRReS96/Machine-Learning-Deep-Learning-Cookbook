{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Regularized Regression Methods**  \n",
        "### *Metody regresji z regularyzacją*\n",
        "\n",
        "---\n",
        "\n",
        "## **English**\n",
        "\n",
        "Regularized regression methods add penalty terms to the standard linear regression cost function to prevent overfitting and improve generalization. These techniques are essential when dealing with high-dimensional data, multicollinearity, or when feature selection is needed.\n",
        "\n",
        "### **Key Concepts**\n",
        "\n",
        "1. **Regularization**: Adding penalty terms to control model complexity\n",
        "2. **Bias-Variance Tradeoff**: Regularization increases bias but reduces variance\n",
        "3. **Feature Selection**: Some methods can automatically select relevant features\n",
        "4. **Hyperparameter Tuning**: Regularization strength needs to be optimized\n",
        "\n",
        "### **Mathematical Foundation**\n",
        "\n",
        "#### **Standard Linear Regression Cost Function**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
        "$$\n",
        "\n",
        "#### **Ridge Regression (L2 Regularization)**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "#### **Lasso Regression (L1 Regularization)**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^{n} |\\theta_j|\n",
        "$$\n",
        "\n",
        "#### **Elastic Net Regression (L1 + L2)**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\rho \\sum_{j=1}^{n} |\\theta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "### **Comparison of Methods**\n",
        "\n",
        "| Method | Penalty | Feature Selection | Multicollinearity | Use Case |\n",
        "|--------|---------|-------------------|-------------------|----------|\n",
        "| **Ridge** | L2 (squared) | No | Handles well | Many correlated features |\n",
        "| **Lasso** | L1 (absolute) | Yes | Picks one from group | Sparse solutions needed |\n",
        "| **Elastic Net** | L1 + L2 | Yes | Handles well | Best of both worlds |\n",
        "| **SGD** | Any | Depends | Depends | Large datasets |\n",
        "\n",
        "### **Advantages and Disadvantages**\n",
        "\n",
        "#### **Ridge Regression**\n",
        "- ✅ Handles multicollinearity well\n",
        "- ✅ Stable solution\n",
        "- ❌ Doesn't perform feature selection\n",
        "- ❌ All features remain in model\n",
        "\n",
        "#### **Lasso Regression**\n",
        "- ✅ Automatic feature selection\n",
        "- ✅ Sparse solutions\n",
        "- ❌ Can be unstable with correlated features\n",
        "- ❌ Arbitrary selection among correlated features\n",
        "\n",
        "#### **Elastic Net**\n",
        "- ✅ Combines benefits of Ridge and Lasso\n",
        "- ✅ Handles correlated features better than Lasso\n",
        "- ✅ Feature selection capability\n",
        "- ❌ Additional hyperparameter to tune\n",
        "\n",
        "#### **SGD Regressor**\n",
        "- ✅ Scales to large datasets\n",
        "- ✅ Memory efficient\n",
        "- ✅ Supports different penalties\n",
        "- ❌ Requires feature scaling\n",
        "- ❌ Sensitive to hyperparameters\n",
        "\n",
        "---\n",
        "\n",
        "## **Polish**\n",
        "\n",
        "Metody regresji z regularyzacją dodają składniki kary do standardowej funkcji kosztu regresji liniowej, aby zapobiec przeuczeniu i poprawić generalizację. Te techniki są niezbędne przy pracy z danymi wysokowymiarowymi, wielokoliniowością lub gdy potrzebna jest selekcja cech.\n",
        "\n",
        "### **Kluczowe pojęcia**\n",
        "\n",
        "1. **Regularyzacja**: Dodawanie składników kary do kontroli złożoności modelu\n",
        "2. **Kompromis bias-wariancja**: Regularyzacja zwiększa bias, ale zmniejsza wariancję\n",
        "3. **Selekcja cech**: Niektóre metody mogą automatycznie wybierać istotne cechy\n",
        "4. **Tuning hiperparametrów**: Siła regularyzacji wymaga optymalizacji\n",
        "\n",
        "### **Podstawy matematyczne**\n",
        "\n",
        "#### **Standardowa funkcja kosztu regresji liniowej**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
        "$$\n",
        "\n",
        "#### **Regresja Ridge (regularyzacja L2)**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "#### **Regresja Lasso (regularyzacja L1)**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^{n} |\\theta_j|\n",
        "$$\n",
        "\n",
        "#### **Regresja Elastic Net (L1 + L2)**\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\rho \\sum_{j=1}^{n} |\\theta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "### **Porównanie metod**\n",
        "\n",
        "| Metoda | Kara | Selekcja cech | Wielokoliniowość | Przypadek użycia |\n",
        "|--------|------|---------------|------------------|------------------|\n",
        "| **Ridge** | L2 (kwadratowa) | Nie | Dobrze radzi | Wiele skorelowanych cech |\n",
        "| **Lasso** | L1 (bezwzględna) | Tak | Wybiera jedną z grupy | Potrzebne rzadkie rozwiązania |\n",
        "| **Elastic Net** | L1 + L2 | Tak | Dobrze radzi | Najlepsze z obu światów |\n",
        "| **SGD** | Dowolna | Zależy | Zależy | Duże zbiory danych |\n",
        "\n",
        "### **Zalety i wady**\n",
        "\n",
        "#### **Regresja Ridge**\n",
        "- ✅ Dobrze radzi z wielokoliniowością\n",
        "- ✅ Stabilne rozwiązanie\n",
        "- ❌ Nie wykonuje selekcji cech\n",
        "- ❌ Wszystkie cechy pozostają w modelu\n",
        "\n",
        "#### **Regresja Lasso**\n",
        "- ✅ Automatyczna selekcja cech\n",
        "- ✅ Rzadkie rozwiązania\n",
        "- ❌ Może być niestabilna ze skorelowanymi cechami\n",
        "- ❌ Arbitralny wybór spośród skorelowanych cech\n",
        "\n",
        "#### **Elastic Net**\n",
        "- ✅ Łączy zalety Ridge i Lasso\n",
        "- ✅ Lepiej radzi ze skorelowanymi cechami niż Lasso\n",
        "- ✅ Możliwość selekcji cech\n",
        "- ❌ Dodatkowy hiperparametr do tuningu\n",
        "\n",
        "#### **SGD Regressor**\n",
        "- ✅ Skaluje się do dużych zbiorów danych\n",
        "- ✅ Efektywny pamięciowo\n",
        "- ✅ Obsługuje różne kary\n",
        "- ❌ Wymaga skalowania cech\n",
        "- ❌ Wrażliwy na hiperparametry\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
