{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **Gaussian Mixture Models (GMM)**  \n",
        "### *Modele mieszanek Gaussowskich*\n",
        "\n",
        "---\n",
        "\n",
        "## **English**\n",
        "\n",
        "Gaussian Mixture Models (GMM) are probabilistic models that assume data comes from a mixture of multiple Gaussian distributions with unknown parameters. GMM is a generalization of K-means clustering that incorporates information about covariance structure and cluster membership probabilities.\n",
        "\n",
        "### **Key Concepts**\n",
        "\n",
        "1. **Mixture Model**: Combination of multiple probability distributions\n",
        "2. **Gaussian Components**: Each cluster is modeled as a Gaussian distribution\n",
        "3. **Soft Clustering**: Points can belong to multiple clusters with different probabilities\n",
        "4. **EM Algorithm**: Expectation-Maximization for parameter estimation\n",
        "5. **Latent Variables**: Hidden cluster assignments\n",
        "\n",
        "### **Mathematical Foundation**\n",
        "\n",
        "#### **Mixture Model**\n",
        "For K components, the probability density function is:\n",
        "$$\n",
        "p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\pi_k$ = mixing coefficient (weight) for component k\n",
        "- $\\mathcal{N}(x | \\mu_k, \\Sigma_k)$ = Gaussian distribution with mean $\\mu_k$ and covariance $\\Sigma_k$\n",
        "- $\\sum_{k=1}^{K} \\pi_k = 1$ and $\\pi_k \\geq 0$\n",
        "\n",
        "#### **Gaussian Distribution**\n",
        "$$\n",
        "\\mathcal{N}(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n",
        "$$\n",
        "\n",
        "#### **Posterior Probability (Responsibility)**\n",
        "$$\n",
        "\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}\n",
        "$$\n",
        "\n",
        "### **EM Algorithm**\n",
        "\n",
        "#### **E-Step (Expectation)**\n",
        "Calculate responsibilities (posterior probabilities):\n",
        "$$\n",
        "\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}\n",
        "$$\n",
        "\n",
        "#### **M-Step (Maximization)**\n",
        "Update parameters:\n",
        "\n",
        "**Mixing coefficients:**\n",
        "$$\n",
        "\\pi_k = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma_{nk}\n",
        "$$\n",
        "\n",
        "**Means:**\n",
        "$$\n",
        "\\mu_k = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} x_n}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
        "$$\n",
        "\n",
        "**Covariances:**\n",
        "$$\n",
        "\\Sigma_k = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} (x_n - \\mu_k)(x_n - \\mu_k)^T}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
        "$$\n",
        "\n",
        "### **Model Selection**\n",
        "\n",
        "#### **Akaike Information Criterion (AIC)**\n",
        "$$\n",
        "AIC = -2 \\ln(L) + 2p\n",
        "$$\n",
        "\n",
        "#### **Bayesian Information Criterion (BIC)**\n",
        "$$\n",
        "BIC = -2 \\ln(L) + p \\ln(N)\n",
        "$$\n",
        "\n",
        "Where L is the likelihood and p is the number of parameters.\n",
        "\n",
        "### **Covariance Types**\n",
        "\n",
        "1. **Full**: Each component has its own general covariance matrix\n",
        "2. **Tied**: All components share the same covariance matrix\n",
        "3. **Diagonal**: Covariance matrices are diagonal (features independent)\n",
        "4. **Spherical**: Covariance matrices are spherical (σ²I)\n",
        "\n",
        "### **Advantages**\n",
        "- **Soft clustering**: Probabilistic cluster assignments\n",
        "- **Flexible cluster shapes**: Can model elliptical clusters\n",
        "- **Probabilistic framework**: Provides uncertainty estimates\n",
        "- **Density estimation**: Can be used for density modeling\n",
        "- **Generative model**: Can generate new samples\n",
        "\n",
        "### **Disadvantages**\n",
        "- **Assumes Gaussian distributions**: May not fit non-Gaussian data\n",
        "- **Sensitive to initialization**: May converge to local optima\n",
        "- **Model selection**: Need to choose number of components\n",
        "- **Computational complexity**: More expensive than K-means\n",
        "- **Curse of dimensionality**: Performance degrades in high dimensions\n",
        "\n",
        "### **Applications**\n",
        "- **Clustering**: Soft clustering with uncertainty\n",
        "- **Dimensionality Reduction**: Combined with PCA\n",
        "- **Anomaly Detection**: Low probability regions as anomalies\n",
        "- **Image Segmentation**: Pixel clustering in images\n",
        "- **Speech Recognition**: Modeling acoustic features\n",
        "- **Finance**: Risk modeling and portfolio optimization\n",
        "\n",
        "### **Comparison with K-Means**\n",
        "\n",
        "| Aspect | GMM | K-Means |\n",
        "|--------|-----|---------|\n",
        "| **Cluster Assignment** | Soft (probabilistic) | Hard (deterministic) |\n",
        "| **Cluster Shape** | Elliptical | Spherical |\n",
        "| **Parameters** | Mean, covariance, weights | Only centroids |\n",
        "| **Complexity** | Higher | Lower |\n",
        "| **Initialization** | More sensitive | Less sensitive |\n",
        "\n",
        "### **Initialization Strategies**\n",
        "\n",
        "1. **Random**: Random parameter initialization\n",
        "2. **K-means**: Use K-means results to initialize\n",
        "3. **K-means++**: Improved K-means initialization\n",
        "4. **Multiple runs**: Run multiple times and select best\n",
        "\n",
        "---\n",
        "\n",
        "## **Polish**\n",
        "\n",
        "Modele mieszanek Gaussowskich (GMM) to modele probabilistyczne, które zakładają, że dane pochodzą z mieszanki wielu rozkładów Gaussowskich o nieznanych parametrach. GMM to uogólnienie klasteryzacji K-means, które uwzględnia informacje o strukturze kowariancji i prawdopodobieństwach przynależności do klastrów.\n",
        "\n",
        "### **Kluczowe pojęcia**\n",
        "\n",
        "1. **Model mieszanki**: Kombinacja wielu rozkładów prawdopodobieństwa\n",
        "2. **Komponenty Gaussowskie**: Każdy klaster jest modelowany jako rozkład Gaussowski\n",
        "3. **Klasteryzacja miękka**: Punkty mogą należeć do wielu klastrów z różnymi prawdopodobieństwami\n",
        "4. **Algorytm EM**: Expectation-Maximization do estymacji parametrów\n",
        "5. **Zmienne ukryte**: Ukryte przypisania klastrów\n",
        "\n",
        "### **Podstawy matematyczne**\n",
        "\n",
        "#### **Model mieszanki**\n",
        "Dla K komponentów, funkcja gęstości prawdopodobieństwa to:\n",
        "$$\n",
        "p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "\n",
        "Gdzie:\n",
        "- $\\pi_k$ = współczynnik mieszania (waga) dla komponentu k\n",
        "- $\\mathcal{N}(x | \\mu_k, \\Sigma_k)$ = rozkład Gaussowski ze średnią $\\mu_k$ i kowariancją $\\Sigma_k$\n",
        "- $\\sum_{k=1}^{K} \\pi_k = 1$ i $\\pi_k \\geq 0$\n",
        "\n",
        "#### **Rozkład Gaussowski**\n",
        "$$\n",
        "\\mathcal{N}(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n",
        "$$\n",
        "\n",
        "#### **Prawdopodobieństwo a posteriori (odpowiedzialność)**\n",
        "$$\n",
        "\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}\n",
        "$$\n",
        "\n",
        "### **Algorytm EM**\n",
        "\n",
        "#### **Krok E (Expectation)**\n",
        "Obliczenie odpowiedzialności (prawdopodobieństwa a posteriori):\n",
        "$$\n",
        "\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}\n",
        "$$\n",
        "\n",
        "#### **Krok M (Maximization)**\n",
        "Aktualizacja parametrów:\n",
        "\n",
        "**Współczynniki mieszania:**\n",
        "$$\n",
        "\\pi_k = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma_{nk}\n",
        "$$\n",
        "\n",
        "**Średnie:**\n",
        "$$\n",
        "\\mu_k = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} x_n}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
        "$$\n",
        "\n",
        "**Kowariancje:**\n",
        "$$\n",
        "\\Sigma_k = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} (x_n - \\mu_k)(x_n - \\mu_k)^T}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
        "$$\n",
        "\n",
        "### **Selekcja modelu**\n",
        "\n",
        "#### **Kryterium informacyjne Akaike (AIC)**\n",
        "$$\n",
        "AIC = -2 \\ln(L) + 2p\n",
        "$$\n",
        "\n",
        "#### **Bayesowskie kryterium informacyjne (BIC)**\n",
        "$$\n",
        "BIC = -2 \\ln(L) + p \\ln(N)\n",
        "$$\n",
        "\n",
        "Gdzie L to wiarygodność, a p to liczba parametrów.\n",
        "\n",
        "### **Typy kowariancji**\n",
        "\n",
        "1. **Pełna**: Każdy komponent ma własną ogólną macierz kowariancji\n",
        "2. **Wiązana**: Wszystkie komponenty dzielą tę samą macierz kowariancji\n",
        "3. **Diagonalna**: Macierze kowariancji są diagonalne (cechy niezależne)\n",
        "4. **Sferyczna**: Macierze kowariancji są sferyczne (σ²I)\n",
        "\n",
        "### **Zalety**\n",
        "- **Klasteryzacja miękka**: Probabilistyczne przypisania klastrów\n",
        "- **Elastyczne kształty klastrów**: Może modelować klastry eliptyczne\n",
        "- **Ramy probabilistyczne**: Zapewnia oszacowania niepewności\n",
        "- **Estymacja gęstości**: Może być używany do modelowania gęstości\n",
        "- **Model generatywny**: Może generować nowe próbki\n",
        "\n",
        "### **Wady**\n",
        "- **Zakłada rozkłady Gaussowskie**: Może nie pasować do danych nie-Gaussowskich\n",
        "- **Wrażliwy na inicjalizację**: Może zbiegać do optimów lokalnych\n",
        "- **Selekcja modelu**: Potrzeba wyboru liczby komponentów\n",
        "- **Złożoność obliczeniowa**: Droższa niż K-means\n",
        "- **Przekleństwo wymiarowości**: Wydajność spada w wysokich wymiarach\n",
        "\n",
        "### **Zastosowania**\n",
        "- **Klasteryzacja**: Miękka klasteryzacja z niepewnością\n",
        "- **Redukcja wymiarowości**: W połączeniu z PCA\n",
        "- **Wykrywanie anomalii**: Regiony o niskim prawdopodobieństwie jako anomalie\n",
        "- **Segmentacja obrazów**: Klasteryzacja pikseli w obrazach\n",
        "- **Rozpoznawanie mowy**: Modelowanie cech akustycznych\n",
        "- **Finanse**: Modelowanie ryzyka i optymalizacja portfela\n",
        "\n",
        "### **Porównanie z K-Means**\n",
        "\n",
        "| Aspekt | GMM | K-Means |\n",
        "|--------|-----|---------|\n",
        "| **Przypisanie klastra** | Miękkie (probabilistyczne) | Twarde (deterministyczne) |\n",
        "| **Kształt klastra** | Eliptyczny | Sferyczny |\n",
        "| **Parametry** | Średnia, kowariancja, wagi | Tylko centroidy |\n",
        "| **Złożoność** | Wyższa | Niższa |\n",
        "| **Inicjalizacja** | Bardziej wrażliwa | Mniej wrażliwa |\n",
        "\n",
        "### **Strategie inicjalizacji**\n",
        "\n",
        "1. **Losowa**: Losowa inicjalizacja parametrów\n",
        "2. **K-means**: Użycie wyników K-means do inicjalizacji\n",
        "3. **K-means++**: Ulepszona inicjalizacja K-means\n",
        "4. **Wielokrotne uruchomienia**: Uruchomienie wielokrotne i wybór najlepszego\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
